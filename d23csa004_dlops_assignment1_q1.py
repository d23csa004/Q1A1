# -*- coding: utf-8 -*-
"""D23CSA004_DLOps_Assignment1_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FaOXsolt-q3y_uK_M1jk1X3uxbN4XXwb
"""

import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import KFold
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import precision_recall_curve, accuracy_score
import numpy as np

# Define the MLP model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(4, 5),
            nn.ReLU(),
            nn.Linear(5, 7),
            nn.ReLU(),
            nn.Linear(7, 3)
        )

    def forward(self, x):
        return self.layers(x)

# Load and preprocess the IRIS dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Convert to PyTorch tensors
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long)

# Implement 10-fold cross-validation
kf = KFold(n_splits=10)

for fold, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=16)
    test_loader = DataLoader(test_dataset, batch_size=16)

    model = MLP()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Training loop
    for epoch in range(100):
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

    # Evaluation
    with torch.no_grad():
        y_true = []
        y_pred = []
        for inputs, targets in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(targets.numpy().tolist())
            y_pred.extend(predicted.numpy().tolist())

        accuracy = accuracy_score(y_true, y_pred)
        print(f'Fold {fold+1}, Accuracy: {accuracy}')

# Tensorboard logging
writer = SummaryWriter()

for epoch in range(100):
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

    writer.add_scalar('Loss/train', loss.item(), epoch)
    writer.add_scalar('Accuracy/train', accuracy, epoch)

writer.close()

import matplotlib.pyplot as plt

# Initialize lists to store losses and accuracies
train_losses = []
train_accuracies = []

for epoch in range(100):
    epoch_loss = 0
    epoch_correct = 0
    epoch_total = 0

    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        _, predicted = torch.max(outputs, 1)
        correct = (predicted == targets).sum().item()
        total = targets.size(0)

        epoch_loss += loss.item()
        epoch_correct += correct
        epoch_total += total

    train_losses.append(epoch_loss / len(train_loader))
    train_accuracies.append(epoch_correct / epoch_total)

# Plot training loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# Plot training accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies)
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()

"""**Data Loading and Preprocessing:** The Iris dataset is loaded and standardized using StandardScaler. This ensures that all features have a mean of 0 and a standard deviation of 1, which can help the model learn more effectively.

**Model Definition:** A Multi-Layer Perceptron (MLP) model is defined with two hidden layers. The first hidden layer has 5 neurons, the second has 7 neurons. The input layer has 4 neurons (corresponding to the 4 features of the Iris dataset), and the output layer has 3 neurons (corresponding to the 3 classes of the Iris dataset).

**Cross-Validation:** A 10-fold cross-validation strategy is implemented. The dataset is split into 10 parts, and the model is trained 10 times, each time using 9 parts for training and 1 part for testing.

**Training:** For each fold, the model is trained for 100 epochs. In each epoch, the model’s parameters are updated to minimize the cross-entropy loss.
Evaluation: After training, the model’s performance is evaluated on the test set. The accuracy of the model is printed for each fold.

**Logging with TensorBoard:** The training loss and accuracy are logged for each epoch using TensorBoard. This allows you to visualize the training process and see how the loss decreases and the accuracy increases over time.


For the precision-recall curve, t-SNE plots, and confusion matrix, you would need to add additional code. The precision-recall curve can be plotted using sklearn’s precision_recall_curve function, t-SNE plots can be created using sklearn’s TSNE class, and the confusion matrix can be computed using sklearn’s confusion_matrix function and then plotted using matplotlib’s imshow function.
"""